{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import dlib\n",
    "import math\n",
    "import tempfile\n",
    "\n",
    "# Initialize face detector and predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('model/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "otherImg = cv2.imread('filters/Carina_Nebula-1.png', cv2.IMREAD_UNCHANGED)\n",
    "if otherImg is None: \n",
    "    print(\"other image not found.\")\n",
    "\n",
    "# Load accessory images\n",
    "sunglasses = cv2.imread('filters/sunglasses.png', cv2.IMREAD_UNCHANGED)\n",
    "if sunglasses is None:\n",
    "    print(\"Sunglasses image not found.\")\n",
    "mustache = cv2.imread('filters/moustache.png', cv2.IMREAD_UNCHANGED)\n",
    "if mustache is None:\n",
    "    print(\"Mustache image not found.\")\n",
    "necktie = cv2.imread('filters/tie.png', cv2.IMREAD_UNCHANGED)\n",
    "if necktie is None:\n",
    "    print(\"Tie image not found.\")\n",
    "hat = cv2.imread('filters/straw_hat.png', cv2.IMREAD_UNCHANGED)\n",
    "if hat is None:\n",
    "    print(\"Hat image not found.\")\n",
    "shi = cv2.imread('filters/prof_shi.png', cv2.IMREAD_UNCHANGED)\n",
    "if shi is None:\n",
    "    print(\"Prof Shi image not found.\")\n",
    "shi2 = cv2.imread('filters/prof_shi2.png', cv2.IMREAD_UNCHANGED)\n",
    "if shi2 is None:\n",
    "    print(\"Prof Shi 2 image not found.\")\n",
    "hat2 = cv2.imread('filters/christmas_hat.png', cv2.IMREAD_UNCHANGED)\n",
    "if hat2 is None:\n",
    "    print(\"Hat 2 image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prof_Shi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shi(image):\n",
    "    global detector, predictor, shi\n",
    "    # Ensure face detection and buddy image are loaded\n",
    "    if detector is None or predictor is None:\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Adjust path\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if not faces:\n",
    "        # No face detected, optionally place buddy in a default corner\n",
    "        # or just return the original image\n",
    "        return image\n",
    "\n",
    "    # Take the first detected face\n",
    "    face = faces[0]\n",
    "    x_start, y_start, x_end, y_end = face.left(), face.top(), face.right(), face.bottom()\n",
    "\n",
    "    face_height = y_end - y_start\n",
    "    # Scale buddy image to have similar height as the face\n",
    "    scale_factor = (face_height / shi.shape[0])* 2.5\n",
    "    new_width = int(shi.shape[1] * scale_factor)\n",
    "    new_height = int(shi.shape[0] * scale_factor)\n",
    "    resized_buddy = cv2.resize(shi, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Decide placement: for example, place buddy to the right of the person's face\n",
    "    # with some horizontal padding. You can also place to the left by changing sign.\n",
    "    padding = 0\n",
    "    x = x = x_start - new_width - padding\n",
    "    y = y_start\n",
    "\n",
    "    # Ensure coordinates are within the image\n",
    "    x1, x2 = max(0, x), min(image.shape[1], x + new_width)\n",
    "    y1, y2 = max(0, y), min(image.shape[0], y + new_height)\n",
    "\n",
    "    # Adjust if buddy goes out of frame. If it doesn't fit, just clip it\n",
    "    sx1, sx2 = x1 - x, x2 - x\n",
    "    sy1, sy2 = y1 - y, y2 - y\n",
    "\n",
    "    buddy_region = resized_buddy[sy1:sy2, sx1:sx2]\n",
    "\n",
    "    if buddy_region.size == 0:\n",
    "        return image  # If no space, skip\n",
    "\n",
    "    # Blend the buddy image\n",
    "    if buddy_region.shape[2] == 4:\n",
    "        alpha_s = buddy_region[:, :, 3] / 255.0\n",
    "        alpha_l = 1.0 - alpha_s\n",
    "        rgb_buddy = buddy_region[:, :, :3]\n",
    "    else:\n",
    "        alpha_s = np.ones((buddy_region.shape[0], buddy_region.shape[1]))\n",
    "        alpha_l = 1.0 - alpha_s\n",
    "        rgb_buddy = buddy_region\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    for c in range(0,3):\n",
    "        roi[:,:,c] = (alpha_s*rgb_buddy[:,:,c] + alpha_l*roi[:,:,c])\n",
    "    image[y1:y2, x1:x2] = roi\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shi2(image):\n",
    "    global detector, predictor, shi2\n",
    "    # Ensure face detection and buddy image are loaded\n",
    "    if detector is None or predictor is None:\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Adjust path\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if not faces:\n",
    "        # No face detected, optionally place buddy in a default corner\n",
    "        # or just return the original image\n",
    "        return image\n",
    "\n",
    "    # Take the first detected face\n",
    "    face = faces[0]\n",
    "    x_start, y_start, x_end, y_end = face.left(), face.top(), face.right(), face.bottom()\n",
    "\n",
    "    face_height = y_end - y_start\n",
    "    # Scale buddy image to have similar height as the face\n",
    "    scale_factor = (face_height / shi2.shape[0])* 2.5\n",
    "    new_width = int(shi2.shape[1] * scale_factor)\n",
    "    new_height = int(shi2.shape[0] * scale_factor)\n",
    "    resized_buddy = cv2.resize(shi2, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Decide placement: for example, place buddy to the right of the person's face\n",
    "    # with some horizontal padding. You can also place to the left by changing sign.\n",
    "    padding = 20\n",
    "    x = x = x_start - new_width - padding\n",
    "    y = y_start\n",
    "\n",
    "    # Ensure coordinates are within the image\n",
    "    x1, x2 = max(0, x), min(image.shape[1], x + new_width)\n",
    "    y1, y2 = max(0, y), min(image.shape[0], y + new_height)\n",
    "\n",
    "    # Adjust if buddy goes out of frame. If it doesn't fit, just clip it\n",
    "    sx1, sx2 = x1 - x, x2 - x\n",
    "    sy1, sy2 = y1 - y, y2 - y\n",
    "\n",
    "    buddy_region = resized_buddy[sy1:sy2, sx1:sx2]\n",
    "\n",
    "    if buddy_region.size == 0:\n",
    "        return image  # If no space, skip\n",
    "\n",
    "    # Blend the buddy image\n",
    "    if buddy_region.shape[2] == 4:\n",
    "        alpha_s = buddy_region[:, :, 3] / 255.0\n",
    "        alpha_l = 1.0 - alpha_s\n",
    "        rgb_buddy = buddy_region[:, :, :3]\n",
    "    else:\n",
    "        alpha_s = np.ones((buddy_region.shape[0], buddy_region.shape[1]))\n",
    "        alpha_l = 1.0 - alpha_s\n",
    "        rgb_buddy = buddy_region\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    for c in range(0,3):\n",
    "        roi[:,:,c] = (alpha_s*rgb_buddy[:,:,c] + alpha_l*roi[:,:,c])\n",
    "    image[y1:y2, x1:x2] = roi\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sunglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sunglass(image):\n",
    "    global sunglasses, detector, predictor\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract eye landmarks\n",
    "        left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)])\n",
    "        right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)])\n",
    "\n",
    "        # Calculate eye centers\n",
    "        left_eye_center = left_eye_points.mean(axis=0).astype(\"int\")\n",
    "        right_eye_center = right_eye_points.mean(axis=0).astype(\"int\")\n",
    "\n",
    "        # Calculate angle between the eye centers\n",
    "        dy = right_eye_center[1] - left_eye_center[1]\n",
    "        dx = right_eye_center[0] - left_eye_center[0]\n",
    "        angle = -math.degrees(math.atan2(dy, dx))\n",
    "\n",
    "        # Calculate the scaling factor based on the distance between the eyes\n",
    "        eye_width = np.linalg.norm(right_eye_center - left_eye_center)\n",
    "        scaling_factor = eye_width / sunglasses.shape[1] * 2.3  # Adjust scaling factor as needed\n",
    "\n",
    "        # Resize the sunglasses image\n",
    "        new_sunglasses_width = int(sunglasses.shape[1] * scaling_factor)\n",
    "        new_sunglasses_height = int(sunglasses.shape[0] * scaling_factor)\n",
    "        resized_sunglasses = cv2.resize(sunglasses, (new_sunglasses_width, new_sunglasses_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Prepare for rotation without cropping\n",
    "        # Calculate the center of the sunglasses image\n",
    "        center = (new_sunglasses_width // 2, new_sunglasses_height // 2)\n",
    "\n",
    "        # Get rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # Calculate the sine and cosine of the rotation angle\n",
    "        abs_cos = abs(M[0, 0])\n",
    "        abs_sin = abs(M[0, 1])\n",
    "\n",
    "        # Compute the new bounding dimensions of the image\n",
    "        bound_w = int(new_sunglasses_width * abs_cos + new_sunglasses_height * abs_sin)\n",
    "        bound_h = int(new_sunglasses_width * abs_sin + new_sunglasses_height * abs_cos)\n",
    "\n",
    "        # Adjust the rotation matrix to consider the translation\n",
    "        M[0, 2] += bound_w / 2 - center[0]\n",
    "        M[1, 2] += bound_h / 2 - center[1]\n",
    "\n",
    "        # Perform the actual rotation and prevent cropping\n",
    "        rotated_sunglasses = cv2.warpAffine(resized_sunglasses, M, (bound_w, bound_h), flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "        # Calculate position to place sunglasses\n",
    "        x = int((left_eye_center[0] + right_eye_center[0]) / 2 - bound_w / 2)\n",
    "        y = int((left_eye_center[1] + right_eye_center[1]) / 2 - bound_h / 2)\n",
    "\n",
    "        # Adjust position for better fit (optional)\n",
    "        y += int(new_sunglasses_height * 0.1)  # Move sunglasses down slightly\n",
    "\n",
    "        # Ensure coordinates are within the image\n",
    "        x1, x2 = max(0, x), min(image.shape[1], x + bound_w)\n",
    "        y1, y2 = max(0, y), min(image.shape[0], y + bound_h)\n",
    "\n",
    "        # Calculate regions\n",
    "        sunglasses_region = rotated_sunglasses[y1 - y:y2 - y, x1 - x:x2 - x]\n",
    "\n",
    "        # Ensure the sunglasses region is not empty\n",
    "        if sunglasses_region.size == 0:\n",
    "            continue  # Skip if the region is empty\n",
    "\n",
    "        # Split channels and compute the mask\n",
    "        if sunglasses_region.shape[2] == 4:\n",
    "            # If the image has an alpha channel\n",
    "            alpha_s = sunglasses_region[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "            # Extract the RGB channels\n",
    "            sunglasses_rgb = sunglasses_region[:, :, :3]\n",
    "        else:\n",
    "            # If the image does not have an alpha channel\n",
    "            alpha_s = np.ones((sunglasses_region.shape[0], sunglasses_region.shape[1]))\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "            sunglasses_rgb = sunglasses_region\n",
    "\n",
    "        # Extract the region of interest from the image\n",
    "        roi = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Blend the sunglasses with the ROI\n",
    "        for c in range(0, 3):\n",
    "            roi[:, :, c] = (alpha_s * sunglasses_rgb[:, :, c] + alpha_l * roi[:, :, c])\n",
    "\n",
    "        # Place the blended region back into the image\n",
    "        image[y1:y2, x1:x2] = roi\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mustache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mustache(image):\n",
    "  global mustache, detector, predictor\n",
    "\n",
    "  # Read the image\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Detect faces\n",
    "  faces = detector(gray)\n",
    "\n",
    "  for face in faces:\n",
    "      landmarks = predictor(gray, face)\n",
    "\n",
    "      # Extract eye landmarks\n",
    "      left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 51)])\n",
    "      right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(52, 55)])\n",
    "\n",
    "      # Calculate eye centers\n",
    "      left_eye_center = left_eye_points.mean(axis=0).astype(\"int\")\n",
    "      right_eye_center = right_eye_points.mean(axis=0).astype(\"int\")\n",
    "\n",
    "      # Calculate angle between the eye centers\n",
    "      dy = right_eye_center[1] - left_eye_center[1]\n",
    "      dx = right_eye_center[0] - left_eye_center[0]\n",
    "      angle = -math.degrees(math.atan2(dy, dx))\n",
    "\n",
    "      # Calculate the scaling factor based on the distance between the eyes\n",
    "      eye_width = np.linalg.norm(right_eye_center - left_eye_center)\n",
    "      scaling_factor = eye_width / mustache.shape[1] * 2.0  # Adjust scaling factor as needed\n",
    "\n",
    "      # Resize the sunglasses image\n",
    "      new_sunglasses_width = int(mustache.shape[1] * scaling_factor)\n",
    "      new_sunglasses_height = int(mustache.shape[0] * scaling_factor)\n",
    "      resized_sunglasses = cv2.resize(mustache, (new_sunglasses_width, new_sunglasses_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "      # Prepare for rotation without cropping\n",
    "      # Calculate the center of the sunglasses image\n",
    "      center = (new_sunglasses_width // 2, new_sunglasses_height // 2)\n",
    "\n",
    "      # Get rotation matrix\n",
    "      M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "      # Calculate the sine and cosine of the rotation angle\n",
    "      abs_cos = abs(M[0, 0])\n",
    "      abs_sin = abs(M[0, 1])\n",
    "\n",
    "      # Compute the new bounding dimensions of the image\n",
    "      bound_w = int(new_sunglasses_width * abs_cos + new_sunglasses_height * abs_sin)\n",
    "      bound_h = int(new_sunglasses_width * abs_sin + new_sunglasses_height * abs_cos)\n",
    "\n",
    "      # Adjust the rotation matrix to consider the translation\n",
    "      M[0, 2] += bound_w / 2 - center[0]\n",
    "      M[1, 2] += bound_h / 2 - center[1]\n",
    "\n",
    "      # Perform the actual rotation and prevent cropping\n",
    "      rotated_sunglasses = cv2.warpAffine(resized_sunglasses, M, (bound_w, bound_h), flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "      # Calculate position to place sunglasses\n",
    "      x = int((left_eye_center[0] + right_eye_center[0]) / 2 - bound_w / 2)\n",
    "      y = int((left_eye_center[1] + right_eye_center[1]) / 2 - bound_h / 2)\n",
    "\n",
    "      # Ensure coordinates are within the image\n",
    "      x1, x2 = max(0, x), min(image.shape[1], x + bound_w)\n",
    "      y1, y2 = max(0, y), min(image.shape[0], y + bound_h)\n",
    "\n",
    "      # Calculate regions\n",
    "      sunglasses_region = rotated_sunglasses[y1 - y:y2 - y, x1 - x:x2 - x]\n",
    "\n",
    "      # Ensure the sunglasses region is not empty\n",
    "      if sunglasses_region.size == 0:\n",
    "          continue  # Skip if the region is empty\n",
    "\n",
    "      # Split channels and compute the mask\n",
    "      if sunglasses_region.shape[2] == 4:\n",
    "          # If the image has an alpha channel\n",
    "          alpha_s = sunglasses_region[:, :, 3] / 255.0\n",
    "          alpha_l = 1.0 - alpha_s\n",
    "          # Extract the RGB channels\n",
    "          sunglasses_rgb = sunglasses_region[:, :, :3]\n",
    "      else:\n",
    "          # If the image does not have an alpha channel\n",
    "          alpha_s = np.ones((sunglasses_region.shape[0], sunglasses_region.shape[1]))\n",
    "          alpha_l = 1.0 - alpha_s\n",
    "          sunglasses_rgb = sunglasses_region\n",
    "\n",
    "      # Extract the region of interest from the image\n",
    "      roi = image[y1:y2, x1:x2]\n",
    "\n",
    "      # Blend the sunglasses with the ROI\n",
    "      for c in range(0, 3):\n",
    "          roi[:, :, c] = (alpha_s * sunglasses_rgb[:, :, c] + alpha_l * roi[:, :, c])\n",
    "\n",
    "      # Place the blended region back into the image\n",
    "      image[y1:y2, x1:x2] = roi\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necktie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_necktie(image):\n",
    "  # FOR THE NECKTIE\n",
    "    global necktie, detector, predictor\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract eye landmarks\n",
    "        left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(7, 8)])\n",
    "        right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(8, 9)])\n",
    "\n",
    "        # Calculate eye centers\n",
    "        left_eye_center = left_eye_points.mean(axis=0).astype(\"int\")\n",
    "        right_eye_center = right_eye_points.mean(axis=0).astype(\"int\")\n",
    "\n",
    "        # Calculate angle between the eye centers\n",
    "        dy = right_eye_center[1] - left_eye_center[1]\n",
    "        dx = right_eye_center[0] - left_eye_center[0]\n",
    "        angle = -math.degrees(math.atan2(dy, dx))\n",
    "\n",
    "        # Calculate the scaling factor based on the distance between the eyes\n",
    "        eye_width = np.linalg.norm(right_eye_center - left_eye_center)\n",
    "        scaling_factor = eye_width / necktie.shape[1] * 10.0  # Adjust scaling factor for tie!!\n",
    "\n",
    "        # Resize the sunglasses image\n",
    "        new_sunglasses_width = int(necktie.shape[1] * scaling_factor)\n",
    "        new_sunglasses_height = int(necktie.shape[0] * scaling_factor)\n",
    "        resized_sunglasses = cv2.resize(necktie, (new_sunglasses_width, new_sunglasses_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Prepare for rotation without cropping\n",
    "        # Calculate the center of the sunglasses image\n",
    "        center = (new_sunglasses_width // 2, 0)\n",
    "\n",
    "        # Get rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # Calculate the sine and cosine of the rotation angle\n",
    "        abs_cos = abs(M[0, 0])\n",
    "        abs_sin = abs(M[0, 1])\n",
    "\n",
    "        # Compute the new bounding dimensions of the image\n",
    "        bound_w = int(new_sunglasses_width * abs_cos + new_sunglasses_height * abs_sin)\n",
    "        bound_h = int(new_sunglasses_width * abs_sin + new_sunglasses_height * abs_cos)\n",
    "\n",
    "        # Adjust the rotation matrix to consider the translation\n",
    "        M[0, 2] += bound_w / 2 - center[0]\n",
    "        M[1, 2] += bound_h / 2 - center[1]\n",
    "\n",
    "        # Perform the actual rotation and prevent cropping\n",
    "        rotated_sunglasses = cv2.warpAffine(resized_sunglasses, M, (bound_w, bound_h), flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "        # Calculate position to place sunglasses\n",
    "        x = int((left_eye_center[0] + right_eye_center[0]) / 2 - bound_w / 2)\n",
    "        y = int((left_eye_center[1] + right_eye_center[1]) / 2 - bound_h / 2)\n",
    "\n",
    "        # Adjust position for better fit (optional)\n",
    "        y += int(new_sunglasses_height * 0.1)  # Move sunglasses down slightly\n",
    "\n",
    "        # Ensure coordinates are within the image\n",
    "        x1, x2 = max(0, x), min(image.shape[1], x + bound_w)\n",
    "        y1, y2 = max(0, y), min(image.shape[0], y + bound_h)\n",
    "\n",
    "        # Calculate regions\n",
    "        sunglasses_region = rotated_sunglasses[y1 - y:y2 - y, x1 - x:x2 - x]\n",
    "\n",
    "        # Ensure the sunglasses region is not empty\n",
    "        if sunglasses_region.size == 0:\n",
    "            continue  # Skip if the region is empty\n",
    "\n",
    "        # Split channels and compute the mask\n",
    "        if sunglasses_region.shape[2] == 4:\n",
    "            # If the image has an alpha channel\n",
    "            alpha_s = sunglasses_region[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "            # Extract the RGB channels\n",
    "            sunglasses_rgb = sunglasses_region[:, :, :3]\n",
    "        else:\n",
    "            # If the image does not have an alpha channel\n",
    "            alpha_s = np.ones((sunglasses_region.shape[0], sunglasses_region.shape[1]))\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "            sunglasses_rgb = sunglasses_region\n",
    "\n",
    "        # Extract the region of interest from the image\n",
    "        roi = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Blend the sunglasses with the ROI\n",
    "        for c in range(0, 3):\n",
    "            roi[:, :, c] = (alpha_s * sunglasses_rgb[:, :, c] + alpha_l * roi[:, :, c])\n",
    "\n",
    "        # Place the blended region back into the image\n",
    "        image[y1:y2, x1:x2] = roi\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Straw Hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hat(image):\n",
    "    \"\"\"\n",
    "    Overlays a hat on detected faces in the image.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.ndarray): The input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The image with hats overlayed.\n",
    "    \"\"\"\n",
    "    global hat, detector, predictor\n",
    "    # Make a copy to avoid modifying the original image\n",
    "    output_image = image.copy()\n",
    "\n",
    "    # Convert image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(output_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        try:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Extract eye landmarks\n",
    "            left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)])\n",
    "            right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)])\n",
    "\n",
    "            # Calculate eye centers\n",
    "            left_eye_center = left_eye_points.mean(axis=0).astype(\"int\")\n",
    "            right_eye_center = right_eye_points.mean(axis=0).astype(\"int\")\n",
    "\n",
    "            # Calculate angle between the eye centers\n",
    "            dy = right_eye_center[1] - left_eye_center[1]\n",
    "            dx = right_eye_center[0] - left_eye_center[0]\n",
    "            angle_rad = np.arctan2(dy, dx)  # Angle in radians\n",
    "            angle_deg = np.degrees(angle_rad)  # Angle in degrees\n",
    "\n",
    "            # Calculate the scaling factor based on the face width\n",
    "            # Store original hat dimensions for scaling\n",
    "            hat_original_width = hat.shape[1]\n",
    "            hat_original_height = hat.shape[0]\n",
    "            face_width = face.right() - face.left()\n",
    "            scaling_factor = face_width / hat_original_width * 1.5  # Adjust multiplier to control hat size\n",
    "\n",
    "            # Resize the hat image\n",
    "            new_hat_width = int(hat_original_width * scaling_factor)\n",
    "            new_hat_height = int(hat_original_height * scaling_factor)\n",
    "            resized_hat = cv2.resize(hat, (new_hat_width, new_hat_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Rotate the hat image\n",
    "            center = (new_hat_width // 2, new_hat_height // 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle_deg, 1.0)\n",
    "\n",
    "            # Compute the sine and cosine of the rotation angle\n",
    "            cos = np.abs(M[0, 0])\n",
    "            sin = np.abs(M[0, 1])\n",
    "\n",
    "            # Compute the new bounding dimensions of the image\n",
    "            nW = int((new_hat_height * sin) + (new_hat_width * cos))\n",
    "            nH = int((new_hat_height * cos) + (new_hat_width * sin))\n",
    "\n",
    "            # Adjust the rotation matrix to take into account translation\n",
    "            M[0, 2] += (nW / 2) - center[0]\n",
    "            M[1, 2] += (nH / 2) - center[1]\n",
    "\n",
    "            # Perform the actual rotation and get the rotated hat\n",
    "            rotated_hat = cv2.warpAffine(resized_hat, M, (nW, nH), flags=cv2.INTER_AREA,\n",
    "                                         borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "            # Compute the midpoint between the eyes\n",
    "            eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
    "                           (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "\n",
    "            # Compute the offset magnitude (adjust as needed)\n",
    "            offset_magnitude = face_width * 0.6  # Adjust this multiplier to control how far up the hat is placed\n",
    "\n",
    "            # Calculate the offset in the rotated coordinate system\n",
    "            dx_offset = -offset_magnitude * np.sin(angle_rad)\n",
    "            dy_offset = -offset_magnitude * np.cos(angle_rad)\n",
    "\n",
    "            # Compute the final position where the hat should be placed\n",
    "            x = int(eyes_center[0] - (nW / 2) + dx_offset)\n",
    "            y = int(eyes_center[1] - (nH / 2) + dy_offset)\n",
    "\n",
    "            # Ensure the coordinates are within the image bounds\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(output_image.shape[1], x + nW)\n",
    "            y2 = min(output_image.shape[0], y + nH)\n",
    "\n",
    "            # Compute the corresponding region in the rotated hat image\n",
    "            hat_x1 = max(0, -x)\n",
    "            hat_y1 = max(0, -y)\n",
    "            hat_x2 = hat_x1 + (x2 - x1)\n",
    "            hat_y2 = hat_y1 + (y2 - y1)\n",
    "\n",
    "            # Crop the rotated hat and the ROI (Region of Interest) to be blended\n",
    "            hat_cropped = rotated_hat[hat_y1:hat_y2, hat_x1:hat_x2]\n",
    "            roi = output_image[y1:y2, x1:x2]\n",
    "\n",
    "            # Check if the overlay and roi are valid\n",
    "            if hat_cropped.shape[0] == 0 or hat_cropped.shape[1] == 0 or roi.shape[0] == 0 or roi.shape[1] == 0:\n",
    "                continue\n",
    "\n",
    "            # Separate the color and alpha channels\n",
    "            if hat_cropped.shape[2] == 4:\n",
    "                alpha_s = hat_cropped[:, :, 3] / 255.0\n",
    "                alpha_l = 1.0 - alpha_s\n",
    "                overlay_bgr = hat_cropped[:, :, :3]\n",
    "            else:\n",
    "                # If no alpha channel, assume full opacity\n",
    "                alpha_s = np.ones((hat_cropped.shape[0], hat_cropped.shape[1]))\n",
    "                alpha_l = 1.0 - alpha_s\n",
    "                overlay_bgr = hat_cropped\n",
    "\n",
    "            # Reshape alpha channels for broadcasting\n",
    "            alpha_s = alpha_s[:, :, np.newaxis]\n",
    "            alpha_l = alpha_l[:, :, np.newaxis]\n",
    "\n",
    "            # Blend the overlay with the ROI using vectorized operations\n",
    "            blended = (alpha_s * overlay_bgr + alpha_l * roi).astype(np.uint8)\n",
    "\n",
    "            # Place the blended region back into the main image\n",
    "            output_image[y1:y2, x1:x2] = blended\n",
    "        except:\n",
    "            return image\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hat2(image):\n",
    "    \"\"\"\n",
    "    Overlays a hat on detected faces in the image.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.ndarray): The input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The image with hats overlayed.\n",
    "    \"\"\"\n",
    "    global hat2, detector, predictor\n",
    "    # Make a copy to avoid modifying the original image\n",
    "    output_image = image.copy()\n",
    "\n",
    "    # Convert image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(output_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        try:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Extract eye landmarks\n",
    "            left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)])\n",
    "            right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)])\n",
    "\n",
    "            # Calculate eye centers\n",
    "            left_eye_center = left_eye_points.mean(axis=0).astype(\"int\")\n",
    "            right_eye_center = right_eye_points.mean(axis=0).astype(\"int\")\n",
    "\n",
    "            # Calculate angle between the eye centers\n",
    "            dy = right_eye_center[1] - left_eye_center[1]\n",
    "            dx = right_eye_center[0] - left_eye_center[0]\n",
    "            angle_rad = np.arctan2(dy, dx)  # Angle in radians\n",
    "            angle_deg = np.degrees(angle_rad)  # Angle in degrees\n",
    "\n",
    "            # Calculate the scaling factor based on the face width\n",
    "            # Store original hat dimensions for scaling\n",
    "            hat_original_width = hat2.shape[1]\n",
    "            hat_original_height = hat2.shape[0]\n",
    "            face_width = face.right() - face.left()\n",
    "            scaling_factor = face_width / hat_original_width * 1.5  # Adjust multiplier to control hat size\n",
    "\n",
    "            # Resize the hat image\n",
    "            new_hat_width = int(hat_original_width * scaling_factor)\n",
    "            new_hat_height = int(hat_original_height * scaling_factor)\n",
    "            resized_hat = cv2.resize(hat2, (new_hat_width, new_hat_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Rotate the hat image\n",
    "            center = (new_hat_width // 2, new_hat_height // 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle_deg, 1.0)\n",
    "\n",
    "            # Compute the sine and cosine of the rotation angle\n",
    "            cos = np.abs(M[0, 0])\n",
    "            sin = np.abs(M[0, 1])\n",
    "\n",
    "            # Compute the new bounding dimensions of the image\n",
    "            nW = int((new_hat_height * sin) + (new_hat_width * cos))\n",
    "            nH = int((new_hat_height * cos) + (new_hat_width * sin))\n",
    "\n",
    "            # Adjust the rotation matrix to take into account translation\n",
    "            M[0, 2] += (nW / 2) - center[0]\n",
    "            M[1, 2] += (nH / 2) - center[1]\n",
    "\n",
    "            # Perform the actual rotation and get the rotated hat\n",
    "            rotated_hat = cv2.warpAffine(resized_hat, M, (nW, nH), flags=cv2.INTER_AREA,\n",
    "                                         borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "            # Compute the midpoint between the eyes\n",
    "            eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
    "                           (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "\n",
    "            # Compute the offset magnitude (adjust as needed)\n",
    "            offset_magnitude = face_width * 0.6  # Adjust this multiplier to control how far up the hat is placed\n",
    "\n",
    "            # Calculate the offset in the rotated coordinate system\n",
    "            dx_offset = -offset_magnitude * np.sin(angle_rad)\n",
    "            dy_offset = -offset_magnitude * np.cos(angle_rad)\n",
    "\n",
    "            # Compute the final position where the hat should be placed\n",
    "            x = int(eyes_center[0] - (nW / 2) + dx_offset)\n",
    "            y = int(eyes_center[1] - (nH / 2) + dy_offset)\n",
    "\n",
    "            # Ensure the coordinates are within the image bounds\n",
    "            x1 = max(0, x)\n",
    "            y1 = max(0, y)\n",
    "            x2 = min(output_image.shape[1], x + nW)\n",
    "            y2 = min(output_image.shape[0], y + nH)\n",
    "\n",
    "            # Compute the corresponding region in the rotated hat image\n",
    "            hat_x1 = max(0, -x)\n",
    "            hat_y1 = max(0, -y)\n",
    "            hat_x2 = hat_x1 + (x2 - x1)\n",
    "            hat_y2 = hat_y1 + (y2 - y1)\n",
    "\n",
    "            # Crop the rotated hat and the ROI (Region of Interest) to be blended\n",
    "            hat_cropped = rotated_hat[hat_y1:hat_y2, hat_x1:hat_x2]\n",
    "            roi = output_image[y1:y2, x1:x2]\n",
    "\n",
    "            # Check if the overlay and roi are valid\n",
    "            if hat_cropped.shape[0] == 0 or hat_cropped.shape[1] == 0 or roi.shape[0] == 0 or roi.shape[1] == 0:\n",
    "                continue\n",
    "\n",
    "            # Separate the color and alpha channels\n",
    "            if hat_cropped.shape[2] == 4:\n",
    "                alpha_s = hat_cropped[:, :, 3] / 255.0\n",
    "                alpha_l = 1.0 - alpha_s\n",
    "                overlay_bgr = hat_cropped[:, :, :3]\n",
    "            else:\n",
    "                # If no alpha channel, assume full opacity\n",
    "                alpha_s = np.ones((hat_cropped.shape[0], hat_cropped.shape[1]))\n",
    "                alpha_l = 1.0 - alpha_s\n",
    "                overlay_bgr = hat_cropped\n",
    "\n",
    "            # Reshape alpha channels for broadcasting\n",
    "            alpha_s = alpha_s[:, :, np.newaxis]\n",
    "            alpha_l = alpha_l[:, :, np.newaxis]\n",
    "\n",
    "            # Blend the overlay with the ROI using vectorized operations\n",
    "            blended = (alpha_s * overlay_bgr + alpha_l * roi).astype(np.uint8)\n",
    "\n",
    "            # Place the blended region back into the main image\n",
    "            output_image[y1:y2, x1:x2] = blended\n",
    "        except:\n",
    "            return image\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def filter_sepia(image):\n",
    "    sepia_filter = np.array([[0.272, 0.534, 0.131],\n",
    "                             [0.349, 0.686, 0.168],\n",
    "                             [0.393, 0.769, 0.189]])\n",
    "    sepia_image = cv2.transform(image, sepia_filter)\n",
    "    sepia_image = np.clip(sepia_image, 0, 255).astype(np.uint8)\n",
    "    return sepia_image\n",
    "\n",
    "def filter_negative(image):\n",
    "    return cv2.bitwise_not(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_filter(image):\n",
    "    kernel = np.array([[0.0030, 0.0133, 0.0219, 0.0133, 0.0030],\n",
    "                       [0.0133, 0.0596, 0.0983, 0.0596, 0.0133],\n",
    "                       [0.0219, 0.0983, 0.1621, 0.0983, 0.0219],\n",
    "                       [0.0133, 0.0596, 0.0983, 0.0596, 0.0133],\n",
    "                       [0.0030, 0.0133, 0.0219, 0.0133, 0.0030]], dtype=\"float\")\n",
    "    out_img = image.copy()\n",
    "\n",
    "    for c in range(3):\n",
    "        channel = out_img[:, :, c]\n",
    "        gauss = signal.convolve2d(channel, kernel, mode='same', boundary='symm')\n",
    "        gauss = np.clip(gauss, 0, 255).astype(\"uint8\")\n",
    "        out_img[:, :, c] = gauss\n",
    "\n",
    "    return out_img\n",
    "\n",
    "def apply_sobel_filter(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gx = np.array([[-1, 0, 1],\n",
    "                   [-2, 0, 2],\n",
    "                   [-1, 0, 1]], dtype=\"int\")\n",
    "    gy = np.array([[1, 2, 1],\n",
    "                   [0, 0, 0],\n",
    "                   [-1, -2, -1]], dtype=\"int\")\n",
    "    sobelx = signal.convolve2d(gray, gx, mode='same', boundary='symm')\n",
    "    sobely = signal.convolve2d(gray, gy, mode='same', boundary='symm')\n",
    "    sobel = np.sqrt(sobelx**2 + sobely**2)\n",
    "    sobel = np.clip(sobel, 0, 255).astype(\"uint8\")\n",
    "    # Return as a BGR image\n",
    "    out_img = cv2.cvtColor(sobel, cv2.COLOR_GRAY2BGR)\n",
    "    return out_img\n",
    "\n",
    "def apply_filmgrain_filter(image):\n",
    "    out_img = image.copy().astype(np.float32)\n",
    "    h, w, _ = out_img.shape\n",
    "    rand_nums = np.random.randint(0, 100, size=(h, w))\n",
    "\n",
    "    # Apply varying brightness changes\n",
    "    # <20: *0.8, <40: *0.6, <60: *0.5, <80: *1.2, <100: *1.4\n",
    "    mask_20 = rand_nums < 20\n",
    "    mask_40 = (rand_nums >= 20) & (rand_nums < 40)\n",
    "    mask_60 = (rand_nums >= 40) & (rand_nums < 60)\n",
    "    mask_80 = (rand_nums >= 60) & (rand_nums < 80)\n",
    "    mask_100 = (rand_nums >= 80)\n",
    "\n",
    "    out_img[mask_20] *= 0.8\n",
    "    out_img[mask_40] *= 0.6\n",
    "    out_img[mask_60] *= 0.5\n",
    "    out_img[mask_80] *= 1.2\n",
    "    out_img[mask_100] *= 1.4\n",
    "\n",
    "    out_img = np.clip(out_img, 0, 255).astype(np.uint8)\n",
    "    return out_img\n",
    "\n",
    "def apply_pixelate_filter(image):\n",
    "    out_img = image.copy()\n",
    "    pixel_size = 5\n",
    "    kernel = np.ones((3, 3)) / 9\n",
    "\n",
    "    # Downsample\n",
    "    downsample = out_img[::pixel_size, ::pixel_size].astype(np.float32)\n",
    "    for c in range(3):\n",
    "        downsample[:, :, c] = signal.convolve2d(downsample[:, :, c], kernel, mode='same', boundary='symm')\n",
    "\n",
    "    # Upsample\n",
    "    pixelated_image = np.repeat(np.repeat(downsample, pixel_size, axis=0), pixel_size, axis=1)\n",
    "    pixelated_image = np.clip(pixelated_image, 0, 255).astype(np.uint8)\n",
    "    return pixelated_image\n",
    "\n",
    "def apply_dither_filter(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    for y in range(1, gray.shape[1]-1):\n",
    "        for x in range(1, gray.shape[0]-1):\n",
    "            oldpixel = gray[x, y]\n",
    "            if oldpixel > 128:\n",
    "                newpixel = 255\n",
    "            else:\n",
    "                newpixel = 0\n",
    "            gray[x, y] = newpixel\n",
    "            quant_error = oldpixel - newpixel\n",
    "            \n",
    "            gray[x + 1, y] += quant_error * (7 / 16)\n",
    "            gray[x - 1, y + 1] += quant_error * (3 / 16)\n",
    "            gray[x, y + 1] += quant_error * (5 / 16)\n",
    "            gray[x + 1, y + 1] += quant_error * (1 / 16)\n",
    "        \n",
    "    image = gray \n",
    "    return image\n",
    "\n",
    "def apply_bitmap_filter(image):\n",
    "    # Using the final \"dither\" from code\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    height, width = gray.shape\n",
    "\n",
    "    for y in range(height - 1):\n",
    "        old_row = gray[y, :]\n",
    "        new_row = np.where(old_row > 96, 255, 0)\n",
    "        gray[y, :] = new_row\n",
    "        quant_error = old_row - new_row\n",
    "\n",
    "        if y + 1 < height:\n",
    "            # bottom-left\n",
    "            if width > 1:\n",
    "                gray[y + 1, :-1] += quant_error[1:] * (3 / 16)\n",
    "            gray[y + 1, :] += quant_error * (5 / 16)\n",
    "            if width > 1:\n",
    "                gray[y + 1, 1:] += quant_error[:-1] * (1 / 16)\n",
    "\n",
    "        if width > 1:\n",
    "            # right neighbor\n",
    "            gray[y, 1:] += quant_error[:-1] * (7 / 16)\n",
    "\n",
    "    dithered = np.clip(gray, 0, 255).astype(np.uint8)\n",
    "    out_img = cv2.cvtColor(dithered, cv2.COLOR_GRAY2BGR)\n",
    "    return out_img\n",
    "\n",
    "def apply_kuwahara_filter(image, window_size=9):\n",
    "    \"\"\"\n",
    "    Apply the Kuwahara filter to the input image using integral images for optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image in BGR format as a NumPy array.\n",
    "    - window_size: Size of the window (must be odd, e.g., 5, 7). Larger sizes capture more context but are slower.\n",
    "    \n",
    "    Returns:\n",
    "    - out_img: Filtered image in BGR format as a NumPy array.\n",
    "    \"\"\"\n",
    "    if window_size % 2 == 0:\n",
    "        raise ValueError(\"Window size must be odd.\")\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    \n",
    "    imageFloat = gray.astype(np.float64)\n",
    "\n",
    "    winsize = 9\n",
    "\n",
    "    #Build subwindows\n",
    "    tmpAvgKerRow = np.hstack((np.ones((1,int((winsize-1)/2+1))),np.zeros((1,int((winsize-1)/2)))))\n",
    "    tmpPadder = np.zeros((1,winsize))\n",
    "    tmpavgker = np.tile(tmpAvgKerRow, (int((winsize-1)/2+1),1))\n",
    "    tmpavgker = np.vstack((tmpavgker, np.tile(tmpPadder, (int((winsize-1)/2),1))))\n",
    "    tmpavgker = tmpavgker/np.sum(tmpavgker)\n",
    "\n",
    "    # tmpavgker is a 'north-west' subwindow (marked as 'a' above)\n",
    "    # we build a vector of convolution kernels for computing average and\n",
    "    # variance\n",
    "    avgker = np.empty((4,winsize,winsize)) # make an empty vector of arrays\n",
    "    avgker[0] = tmpavgker\t\t\t# North-west (a)\n",
    "    avgker[1] = np.fliplr(tmpavgker)\t# North-east (b)\n",
    "    avgker[2] = np.flipud(tmpavgker)\t# South-west (c)\n",
    "    avgker[3] = np.fliplr(avgker[2])\t# South-east (d)\n",
    "    \n",
    "    # Create a pixel-by-pixel square of the image\n",
    "    squaredImg = imageFloat**2\n",
    "    \n",
    "    # preallocate these arrays to make it apparently %15 faster\n",
    "    avgs = np.zeros([4, imageFloat.shape[0],imageFloat.shape[1]])\n",
    "    stddevs = avgs.copy()\n",
    "\n",
    "    # Calculation of averages and variances on subwindows\n",
    "    for k in range(4):\n",
    "        avgs[k] = signal.convolve2d(imageFloat, avgker[k],mode='same') \t    # mean on subwindow\n",
    "        stddevs[k] = signal.convolve2d(squaredImg, avgker[k],mode='same')  # mean of squares on subwindow\n",
    "        stddevs[k] = stddevs[k]-avgs[k]**2 \t\t\t    # variance on subwindow\n",
    "    \n",
    "    # Choice of index with minimum variance\n",
    "    indices = np.argmin(stddevs,0) # returns index of subwindow with smallest variance\n",
    "\n",
    "    # Building the filtered image (with nested for loops)\n",
    "    filtered = np.zeros(image.shape)\n",
    "    for row in range(image.shape[0]):\n",
    "        for col in range(image.shape[1]):\n",
    "            filtered[row,col] = avgs[indices[row,col], row,col]\n",
    "\n",
    "    #filtered=filtered.astype(np.uint8)\n",
    "    image = filtered.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def apply_blendframes_filter(image, alpha=0.8):\n",
    "    global otherImg\n",
    "    # otherImg must be same size or we resize it\n",
    "    if otherImg.shape[:2] != image.shape[:2]:\n",
    "        otherImg = cv2.resize(otherImg, (image.shape[1], image.shape[0]))\n",
    "    out_img = image.astype(np.float32)\n",
    "    out_img = out_img * alpha + otherImg.astype(np.float32) * (1 - alpha)\n",
    "    out_img = np.clip(out_img, 0, 255).astype(np.uint8)\n",
    "    return out_img\n",
    "\n",
    "def apply_brokenvcr_filter(image):\n",
    "    # Adapted from provided code\n",
    "    PI = 3.14159\n",
    "    test = (0.25 * 0.5) % 1.0\n",
    "    warped_image = image.astype(np.float32)\n",
    "    h, w, _ = warped_image.shape\n",
    "    y_coords = np.linspace(0, 1, h)\n",
    "    y_diff = np.abs(y_coords - test)\n",
    "    mask = y_diff < 0.5\n",
    "\n",
    "    for y in range(h):\n",
    "        if mask[y]:\n",
    "            warped_image[y, :, 1] += 0.9 * np.tan((y / h) * 150.0)\n",
    "    warped_image = np.clip(warped_image, 0, 255)\n",
    "\n",
    "    red_copy = np.copy(warped_image[:, :, 0])\n",
    "    if h > 10 and w > 10:\n",
    "        red_copy[10:, 10:] = warped_image[:-10, :-10, 0]\n",
    "        red_copy[:10, :] = 0\n",
    "        red_copy[:, :10] = 0\n",
    "\n",
    "    green_copy = np.copy(warped_image[:, :, 1])\n",
    "    if h > 5 and w > 5:\n",
    "        green_copy[5:, 5:] = warped_image[:-5, :-5, 1]\n",
    "        green_copy[:5, :] = 0\n",
    "        green_copy[:, :5] = 0\n",
    "\n",
    "    shifted_image = warped_image.copy()\n",
    "    shifted_image[:, :, 0] = red_copy\n",
    "    shifted_image[:, :, 1] = green_copy\n",
    "\n",
    "    out_img = np.clip(shifted_image, 0, 255).astype(np.uint8)\n",
    "    return out_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade_filters = {\n",
    "    \"None\": lambda img: img,\n",
    "    \"Sepia\": filter_sepia,\n",
    "    \"Negative\": filter_negative,\n",
    "    \"Gaussian\": apply_gaussian_filter,\n",
    "    \"Edges\": apply_sobel_filter,\n",
    "    \"Filmgrain\": apply_filmgrain_filter,\n",
    "    \"Pixelate\": apply_pixelate_filter,\n",
    "    \"Dither\": apply_dither_filter,\n",
    "    \"Bit Map\": apply_bitmap_filter,\n",
    "    \"Kuwahara\": apply_kuwahara_filter,\n",
    "    \"BlendFrames\": apply_blendframes_filter,  # Remember to pass otherImg as arg in UI code\n",
    "    \"Brokenvcr\": apply_brokenvcr_filter\n",
    "}\n",
    "\n",
    "filter_functions = {\n",
    "    \"None\": lambda img: img,\n",
    "    \"Sunglass\": filter_sunglass,\n",
    "    \"Mustache\": filter_mustache,\n",
    "    \"Necktie\": filter_necktie,\n",
    "    \"Straw Hat\": filter_hat,\n",
    "    \"Prof_Shi\": filter_shi,\n",
    "    \"Prof_Shi2\": filter_shi2,\n",
    "    \"Christmas Hat\": filter_hat2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nehat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout,\n",
    "                             QLabel, QPushButton, QComboBox, QFileDialog, QMessageBox)\n",
    "from PyQt5.QtCore import QTimer, Qt\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Overlay + Shade Filters - PyQt\")\n",
    "        self.resize(800, 600)\n",
    "\n",
    "        self.input_mode = \"Image\"\n",
    "        self.current_image = None\n",
    "        self.current_video_path = None\n",
    "        self.cap = None\n",
    "        self.timer = QTimer()\n",
    "\n",
    "        self.processed_video_output_path = None\n",
    "        self.last_webcam_frame = None\n",
    "        self.flip_webcam = False\n",
    "\n",
    "        # Combos for overlay and shade filters\n",
    "        self.overlay_combo = QComboBox()\n",
    "        self.overlay_combo.addItems(list(filter_functions.keys()))\n",
    "        \n",
    "        self.shade_combo = QComboBox()\n",
    "        self.shade_combo.addItems(list(shade_filters.keys()))\n",
    "\n",
    "        self.mode_combo = QComboBox()\n",
    "        self.mode_combo.addItems([\"Image\", \"Video\", \"Webcam\"])\n",
    "        self.mode_combo.currentTextChanged.connect(self.on_mode_changed)\n",
    "\n",
    "        self.load_button = QPushButton(\"Load\")\n",
    "        self.load_button.clicked.connect(self.on_load_clicked)\n",
    "\n",
    "        self.apply_button = QPushButton(\"Apply Filters\")\n",
    "        self.apply_button.clicked.connect(self.on_apply_filters)\n",
    "        \n",
    "        self.save_button = QPushButton(\"Save\")\n",
    "        self.save_button.setEnabled(False)\n",
    "        self.save_button.clicked.connect(self.on_save_clicked)\n",
    "\n",
    "        self.take_snapshot_button = QPushButton(\"Take Snapshot\")\n",
    "        self.take_snapshot_button.setEnabled(False)\n",
    "        self.take_snapshot_button.clicked.connect(self.on_take_snapshot_clicked)\n",
    "\n",
    "        self.flip_button = QPushButton(\"Flip Horizontal\")\n",
    "        self.flip_button.setEnabled(False)\n",
    "        self.flip_button.clicked.connect(self.on_flip_clicked)\n",
    "\n",
    "        self.image_label = QLabel(\"No input loaded.\")\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "        self.image_label.setStyleSheet(\"background-color: #aaa;\")\n",
    "\n",
    "        top_layout = QHBoxLayout()\n",
    "        top_layout.addWidget(QLabel(\"Input Type:\"))\n",
    "        top_layout.addWidget(self.mode_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(QLabel(\"Overlay Filter:\"))\n",
    "        top_layout.addWidget(self.overlay_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(QLabel(\"Shade Filter:\"))\n",
    "        top_layout.addWidget(self.shade_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(self.load_button)\n",
    "        top_layout.addWidget(self.apply_button)\n",
    "        top_layout.addWidget(self.take_snapshot_button)\n",
    "        top_layout.addWidget(self.flip_button)\n",
    "        top_layout.addWidget(self.save_button)\n",
    "\n",
    "        main_layout = QVBoxLayout()\n",
    "        main_layout.addLayout(top_layout)\n",
    "        main_layout.addWidget(self.image_label, stretch=1)\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "        self.update_ui_for_mode()\n",
    "\n",
    "    def on_mode_changed(self, text):\n",
    "        self.input_mode = text\n",
    "        self.update_ui_for_mode()\n",
    "\n",
    "    def update_ui_for_mode(self):\n",
    "        self.save_button.setEnabled(False)\n",
    "        self.take_snapshot_button.setEnabled(False)\n",
    "        self.flip_button.setEnabled(False)\n",
    "        if self.input_mode == \"Image\":\n",
    "            self.load_button.setText(\"Load Image\")\n",
    "            self.stop_webcam()\n",
    "            self.image_label.setText(\"No image loaded.\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "        elif self.input_mode == \"Video\":\n",
    "            self.load_button.setText(\"Load Video\")\n",
    "            self.stop_webcam()\n",
    "            self.image_label.setText(\"No video loaded.\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            self.load_button.setText(\"Start Webcam\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "            self.start_webcam()\n",
    "            self.take_snapshot_button.setEnabled(True)\n",
    "            self.flip_button.setEnabled(True)\n",
    "\n",
    "    def on_load_clicked(self):\n",
    "        self.save_button.setEnabled(False)\n",
    "        if self.input_mode == \"Image\":\n",
    "            path, _ = QFileDialog.getOpenFileName(self, \"Select Image\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                img = cv2.imread(path)\n",
    "                if img is not None:\n",
    "                    self.current_image = img\n",
    "                    self.show_image(img)\n",
    "                else:\n",
    "                    QMessageBox.warning(self, \"Error\", \"Failed to load image.\")\n",
    "        elif self.input_mode == \"Video\":\n",
    "            path, _ = QFileDialog.getOpenFileName(self, \"Select Video\", \"\", \"Video Files (*.mp4 *.avi *.mov)\")\n",
    "            if path:\n",
    "                self.current_video_path = path\n",
    "                cap = cv2.VideoCapture(path)\n",
    "                ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                if ret:\n",
    "                    self.show_image(frame)\n",
    "                else:\n",
    "                    QMessageBox.warning(self, \"Error\", \"Failed to load video.\")\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            # Toggle webcam on/off\n",
    "            if self.cap is None:\n",
    "                self.start_webcam()\n",
    "            else:\n",
    "                self.stop_webcam()\n",
    "\n",
    "    def start_webcam(self):\n",
    "        if self.cap is not None:\n",
    "            return\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        if not self.cap.isOpened():\n",
    "            QMessageBox.warning(self, \"Error\", \"Failed to access webcam.\")\n",
    "            self.cap = None\n",
    "            return\n",
    "        self.timer.timeout.connect(self.update_webcam_frame)\n",
    "        self.timer.start(30)\n",
    "        self.load_button.setText(\"Stop Webcam\")\n",
    "        self.take_snapshot_button.setEnabled(True)\n",
    "        self.flip_button.setEnabled(True)\n",
    "\n",
    "    def stop_webcam(self):\n",
    "        if self.cap is not None:\n",
    "            self.timer.stop()\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "            self.load_button.setText(\"Start Webcam\")\n",
    "            self.image_label.setText(\"Webcam stopped.\")\n",
    "            self.take_snapshot_button.setEnabled(False)\n",
    "            self.flip_button.setEnabled(False)\n",
    "\n",
    "    def update_webcam_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return\n",
    "        if self.flip_webcam:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "        frame = self.apply_filters_to_frame(frame)\n",
    "        self.show_image(frame)\n",
    "        self.last_webcam_frame = frame\n",
    "\n",
    "    def apply_filters_to_frame(self, frame):\n",
    "        # Apply overlay filter first\n",
    "        overlay_name = self.overlay_combo.currentText()\n",
    "        overlay_func = filter_functions.get(overlay_name, lambda x: x)\n",
    "        frame = overlay_func(frame)\n",
    "\n",
    "        # Then apply shade filter\n",
    "        shade_name = self.shade_combo.currentText()\n",
    "        shade_func = shade_filters.get(shade_name, lambda x: x)\n",
    "        frame = shade_func(frame)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def show_image(self, image):\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        qimg = QImage(rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        pix = QPixmap.fromImage(qimg)\n",
    "        self.image_label.setPixmap(pix.scaled(self.image_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))\n",
    "\n",
    "    def on_apply_filters(self):\n",
    "        if self.input_mode == \"Image\":\n",
    "            if self.current_image is None:\n",
    "                QMessageBox.warning(self, \"No Image\", \"Load an image first.\")\n",
    "                return\n",
    "            processed = self.apply_filters_to_frame(self.current_image)\n",
    "            self.show_image(processed)\n",
    "            self.current_image = processed\n",
    "            self.save_button.setEnabled(True)\n",
    "        elif self.input_mode == \"Video\":\n",
    "            if self.current_video_path is None:\n",
    "                QMessageBox.warning(self, \"No Video\", \"Load a video first.\")\n",
    "                return\n",
    "            output_path = self.process_video_file(self.current_video_path)\n",
    "            if output_path:\n",
    "                # Show first frame of processed video\n",
    "                cap = cv2.VideoCapture(output_path)\n",
    "                ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                if ret:\n",
    "                    self.show_image(frame)\n",
    "                QMessageBox.information(self, \"Done\", f\"Video processed at:\\n{output_path}\")\n",
    "                self.processed_video_output_path = output_path\n",
    "                self.save_button.setEnabled(True)\n",
    "            else:\n",
    "                QMessageBox.warning(self, \"Error\", \"Video processing failed.\")\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            QMessageBox.information(self, \"Info\", \"Filter applied in real-time to webcam feed.\\nUse 'Take Snapshot' to capture a frame.\")\n",
    "\n",
    "    def process_video_file(self, video_path):\n",
    "        overlay_name = self.overlay_combo.currentText()\n",
    "        overlay_func = filter_functions.get(overlay_name, lambda x: x)\n",
    "\n",
    "        shade_name = self.shade_combo.currentText()\n",
    "        shade_func = shade_filters.get(shade_name, lambda x: x)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        output_path = os.path.join(temp_dir, 'processed_video_output.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = overlay_func(frame)\n",
    "            frame = shade_func(frame)\n",
    "            if len(frame.shape) == 2:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "            out.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        return output_path\n",
    "\n",
    "    def on_save_clicked(self):\n",
    "        if self.input_mode == \"Image\" and self.current_image is not None:\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Image\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                cv2.imwrite(path, self.current_image)\n",
    "                QMessageBox.information(self, \"Saved\", f\"Image saved at:\\n{path}\")\n",
    "        elif self.input_mode == \"Video\" and self.processed_video_output_path is not None:\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Video\", \"\", \"Video Files (*.mp4 *.avi)\")\n",
    "            if path:\n",
    "                try:\n",
    "                    shutil.copyfile(self.processed_video_output_path, path)\n",
    "                    QMessageBox.information(self, \"Saved\", f\"Video saved at:\\n{path}\")\n",
    "                except Exception as e:\n",
    "                    QMessageBox.warning(self, \"Error\", f\"Failed to save video.\\n{e}\")\n",
    "        elif self.input_mode == \"Webcam\" and self.current_image is not None:\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Snapshot\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                cv2.imwrite(path, self.current_image)\n",
    "                QMessageBox.information(self, \"Saved\", f\"Snapshot saved at:\\n{path}\")\n",
    "        else:\n",
    "            QMessageBox.information(self, \"Info\", \"Nothing to save at this moment.\")\n",
    "\n",
    "    def on_take_snapshot_clicked(self):\n",
    "        # If we have last_webcam_frame stored, treat it as current_image.\n",
    "        if self.input_mode == \"Webcam\" and self.last_webcam_frame is not None:\n",
    "            self.current_image = self.last_webcam_frame.copy()\n",
    "            self.save_button.setEnabled(True)\n",
    "            QMessageBox.information(self, \"Snapshot\", \"Snapshot taken and ready to save!\")\n",
    "        else:\n",
    "            QMessageBox.information(self, \"Info\", \"No webcam frame available to snapshot.\")\n",
    "\n",
    "    def on_flip_clicked(self):\n",
    "        self.flip_webcam = not self.flip_webcam\n",
    "        if self.flip_webcam:\n",
    "            self.flip_button.setText(\"Unflip\")\n",
    "        else:\n",
    "            self.flip_button.setText(\"Flip Horizontal\")\n",
    "\n",
    "app = QApplication(sys.argv)\n",
    "win = MainWindow()\n",
    "win.show()\n",
    "sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 10:52:11.227 Python[53732:20969702] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "2024-12-07 10:52:13.527 Python[53732:20969702] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "/var/folders/gg/w5v2fbr54774543fcpcync1c0000gn/T/ipykernel_53732/2712805928.py:224: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  quant_error = oldpixel - newpixel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout,\n",
    "                             QLabel, QPushButton, QComboBox, QFileDialog, QMessageBox)\n",
    "from PyQt5.QtCore import QTimer, Qt\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Overlay + Shade Filters - PyQt\")\n",
    "        self.resize(800, 600)\n",
    "\n",
    "        self.input_mode = \"Image\"\n",
    "        self.current_image = None\n",
    "        self.current_video_path = None\n",
    "        self.cap = None\n",
    "        self.timer = QTimer()\n",
    "\n",
    "        self.processed_video_output_path = None\n",
    "        self.last_webcam_frame = None\n",
    "        self.flip_webcam = False\n",
    "\n",
    "        # Combos for overlay and shade filters\n",
    "        self.overlay_combo = QComboBox()\n",
    "        self.overlay_combo.addItems(list(filter_functions.keys()))\n",
    "        \n",
    "        self.shade_combo = QComboBox()\n",
    "        self.shade_combo.addItems(list(shade_filters.keys()))\n",
    "\n",
    "        self.mode_combo = QComboBox()\n",
    "        self.mode_combo.addItems([\"Image\", \"Video\", \"Webcam\"])\n",
    "        self.mode_combo.currentTextChanged.connect(self.on_mode_changed)\n",
    "\n",
    "        self.load_button = QPushButton(\"Load\")\n",
    "        self.load_button.clicked.connect(self.on_load_clicked)\n",
    "\n",
    "        self.apply_button = QPushButton(\"Apply Filters\")\n",
    "        self.apply_button.clicked.connect(self.on_apply_filters)\n",
    "        \n",
    "        self.save_button = QPushButton(\"Save\")\n",
    "        self.save_button.setEnabled(False)\n",
    "        self.save_button.clicked.connect(self.on_save_clicked)\n",
    "\n",
    "        self.take_snapshot_button = QPushButton(\"Take Snapshot\")\n",
    "        self.take_snapshot_button.setEnabled(False)\n",
    "        self.take_snapshot_button.clicked.connect(self.on_take_snapshot_clicked)\n",
    "\n",
    "        self.flip_button = QPushButton(\"Flip Horizontal\")\n",
    "        self.flip_button.setEnabled(False)\n",
    "        self.flip_button.clicked.connect(self.on_flip_clicked)\n",
    "\n",
    "        self.image_label = QLabel(\"No input loaded.\")\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "        self.image_label.setStyleSheet(\"background-color: #aaa;\")\n",
    "\n",
    "        top_layout = QHBoxLayout()\n",
    "        top_layout.addWidget(QLabel(\"Input Type:\"))\n",
    "        top_layout.addWidget(self.mode_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(QLabel(\"Overlay Filter:\"))\n",
    "        top_layout.addWidget(self.overlay_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(QLabel(\"Shade Filter:\"))\n",
    "        top_layout.addWidget(self.shade_combo)\n",
    "        top_layout.addSpacing(20)\n",
    "\n",
    "        top_layout.addWidget(self.load_button)\n",
    "        top_layout.addWidget(self.apply_button)\n",
    "        top_layout.addWidget(self.take_snapshot_button)\n",
    "        top_layout.addWidget(self.flip_button)\n",
    "        top_layout.addWidget(self.save_button)\n",
    "\n",
    "        main_layout = QVBoxLayout()\n",
    "        main_layout.addLayout(top_layout)\n",
    "        main_layout.addWidget(self.image_label, stretch=1)\n",
    "\n",
    "        self.setLayout(main_layout)\n",
    "        self.update_ui_for_mode()\n",
    "\n",
    "        # Initialize a separate timer for video playback\n",
    "        self.video_timer = QTimer()\n",
    "        self.video_timer.timeout.connect(self.update_video_frame)\n",
    "\n",
    "        # VideoWriter object for saving processed video\n",
    "        self.video_out = None\n",
    "\n",
    "    def on_mode_changed(self, text):\n",
    "        self.input_mode = text\n",
    "        self.update_ui_for_mode()\n",
    "\n",
    "    def update_ui_for_mode(self):\n",
    "        self.save_button.setEnabled(False)\n",
    "        self.take_snapshot_button.setEnabled(False)\n",
    "        self.flip_button.setEnabled(False)\n",
    "        if self.input_mode == \"Image\":\n",
    "            self.load_button.setText(\"Load Image\")\n",
    "            self.stop_webcam()\n",
    "            self.image_label.setText(\"No image loaded.\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "        elif self.input_mode == \"Video\":\n",
    "            self.load_button.setText(\"Load Video\")\n",
    "            self.stop_webcam()\n",
    "            self.image_label.setText(\"No video loaded.\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            self.load_button.setText(\"Start Webcam\")\n",
    "            self.current_image = None\n",
    "            self.current_video_path = None\n",
    "            self.start_webcam()\n",
    "            self.take_snapshot_button.setEnabled(True)\n",
    "            self.flip_button.setEnabled(True)\n",
    "\n",
    "    def on_load_clicked(self):\n",
    "        self.save_button.setEnabled(False)\n",
    "        if self.input_mode == \"Image\":\n",
    "            path, _ = QFileDialog.getOpenFileName(self, \"Select Image\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                img = cv2.imread(path)\n",
    "                if img is not None:\n",
    "                    self.current_image = img\n",
    "                    self.show_image(img)\n",
    "                else:\n",
    "                    QMessageBox.warning(self, \"Error\", \"Failed to load image.\")\n",
    "        elif self.input_mode == \"Video\":\n",
    "            path, _ = QFileDialog.getOpenFileName(self, \"Select Video\", \"\", \"Video Files (*.mp4 *.avi *.mov)\")\n",
    "            if path:\n",
    "                self.current_video_path = path\n",
    "                cap = cv2.VideoCapture(path)\n",
    "                ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                if ret:\n",
    "                    self.show_image(frame)\n",
    "                else:\n",
    "                    QMessageBox.warning(self, \"Error\", \"Failed to load video.\")\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            # Toggle webcam on/off\n",
    "            if self.cap is None:\n",
    "                self.start_webcam()\n",
    "            else:\n",
    "                self.stop_webcam()\n",
    "\n",
    "    def start_webcam(self):\n",
    "        if self.cap is not None:\n",
    "            return\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        if not self.cap.isOpened():\n",
    "            QMessageBox.warning(self, \"Error\", \"Failed to access webcam.\")\n",
    "            self.cap = None\n",
    "            return\n",
    "        self.timer.timeout.connect(self.update_webcam_frame)\n",
    "        self.timer.start(30)\n",
    "        self.load_button.setText(\"Stop Webcam\")\n",
    "        self.take_snapshot_button.setEnabled(True)\n",
    "        self.flip_button.setEnabled(True)\n",
    "\n",
    "    def stop_webcam(self):\n",
    "        if self.cap is not None:\n",
    "            self.timer.stop()\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "            self.load_button.setText(\"Start Webcam\")\n",
    "            self.image_label.setText(\"Webcam stopped.\")\n",
    "            self.take_snapshot_button.setEnabled(False)\n",
    "            self.flip_button.setEnabled(False)\n",
    "\n",
    "    def update_webcam_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return\n",
    "        if self.flip_webcam:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "        frame = self.apply_filters_to_frame(frame)\n",
    "        self.show_image(frame)\n",
    "        self.last_webcam_frame = frame\n",
    "\n",
    "    def apply_filters_to_frame(self, frame):\n",
    "        # Apply overlay filter first\n",
    "        overlay_name = self.overlay_combo.currentText()\n",
    "        overlay_func = filter_functions.get(overlay_name, lambda x: x)\n",
    "        frame = overlay_func(frame)\n",
    "\n",
    "        # Then apply shade filter\n",
    "        shade_name = self.shade_combo.currentText()\n",
    "        shade_func = shade_filters.get(shade_name, lambda x: x)\n",
    "        frame = shade_func(frame)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def show_image(self, image):\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        qimg = QImage(rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        pix = QPixmap.fromImage(qimg)\n",
    "        self.image_label.setPixmap(pix.scaled(self.image_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))\n",
    "\n",
    "    def on_apply_filters(self):\n",
    "        if self.input_mode == \"Image\":\n",
    "            if self.current_image is None:\n",
    "                QMessageBox.warning(self, \"No Image\", \"Load an image first.\")\n",
    "                return\n",
    "            processed = self.apply_filters_to_frame(self.current_image)\n",
    "            self.show_image(processed)\n",
    "            self.current_image = processed\n",
    "            self.save_button.setEnabled(True)\n",
    "\n",
    "        elif self.input_mode == \"Video\":\n",
    "            if self.current_video_path is None:\n",
    "                QMessageBox.warning(self, \"No Video\", \"Load a video first.\")\n",
    "                return\n",
    "\n",
    "            # Open the video file\n",
    "            self.cap_video = cv2.VideoCapture(self.current_video_path)\n",
    "            if not self.cap_video.isOpened():\n",
    "                QMessageBox.warning(self, \"Error\", \"Failed to open video.\")\n",
    "                return\n",
    "\n",
    "            # Get video properties\n",
    "            frame_width = int(self.cap_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            frame_height = int(self.cap_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = self.cap_video.get(cv2.CAP_PROP_FPS)\n",
    "            if fps == 0:\n",
    "                fps = 30  # Fallback FPS\n",
    "\n",
    "            # Define the codec and create VideoWriter object\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            output_path = os.path.join(temp_dir, 'processed_video_output.mp4')\n",
    "            self.video_out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "            # Start the video playback timer\n",
    "            self.video_timer.start(int(1000 / fps))\n",
    "\n",
    "            QMessageBox.information(self, \"Info\", \"Started video playback with filters.\")\n",
    "            self.save_button.setEnabled(True)\n",
    "\n",
    "        elif self.input_mode == \"Webcam\":\n",
    "            QMessageBox.information(self, \"Info\", \"Filter applied in real-time to webcam feed.\\nUse 'Take Snapshot' to capture a frame.\")\n",
    "\n",
    "    def process_video_file(self, video_path):\n",
    "        overlay_name = self.overlay_combo.currentText()\n",
    "        overlay_func = filter_functions.get(overlay_name, lambda x: x)\n",
    "\n",
    "        shade_name = self.shade_combo.currentText()\n",
    "        shade_func = shade_filters.get(shade_name, lambda x: x)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        output_path = os.path.join(temp_dir, 'processed_video_output.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = overlay_func(frame)\n",
    "            frame = shade_func(frame)\n",
    "            if len(frame.shape) == 2:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "            out.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        return output_path\n",
    "\n",
    "    def on_save_clicked(self):\n",
    "        if self.input_mode == \"Image\" and self.current_image is not None:\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Image\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                cv2.imwrite(path, self.current_image)\n",
    "                QMessageBox.information(self, \"Saved\", f\"Image saved at:\\n{path}\")\n",
    "\n",
    "        elif self.input_mode == \"Video\":\n",
    "            if self.processed_video_output_path is None:\n",
    "                QMessageBox.warning(self, \"No Processed Video\", \"There is no processed video to save.\")\n",
    "                return\n",
    "\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Video\", \"\", \"Video Files (*.mp4 *.avi)\")\n",
    "            if path:\n",
    "                try:\n",
    "                    shutil.copyfile(self.processed_video_output_path, path)\n",
    "                    QMessageBox.information(self, \"Saved\", f\"Video saved at:\\n{path}\")\n",
    "                except Exception as e:\n",
    "                    QMessageBox.warning(self, \"Error\", f\"Failed to save video.\\n{e}\")\n",
    "\n",
    "        elif self.input_mode == \"Webcam\" and self.current_image is not None:\n",
    "            path, _ = QFileDialog.getSaveFileName(self, \"Save Snapshot\", \"\", \"Images (*.png *.jpg *.jpeg *.bmp)\")\n",
    "            if path:\n",
    "                cv2.imwrite(path, self.current_image)\n",
    "                QMessageBox.information(self, \"Saved\", f\"Snapshot saved at:\\n{path}\")\n",
    "        else:\n",
    "            QMessageBox.information(self, \"Info\", \"Nothing to save at this moment.\")\n",
    "\n",
    "    def on_take_snapshot_clicked(self):\n",
    "        # If we have last_webcam_frame stored, treat it as current_image.\n",
    "        if self.input_mode == \"Webcam\" and self.last_webcam_frame is not None:\n",
    "            self.current_image = self.last_webcam_frame.copy()\n",
    "            self.save_button.setEnabled(True)\n",
    "            QMessageBox.information(self, \"Snapshot\", \"Snapshot taken and ready to save!\")\n",
    "        else:\n",
    "            QMessageBox.information(self, \"Info\", \"No webcam frame available to snapshot.\")\n",
    "\n",
    "    def on_flip_clicked(self):\n",
    "        self.flip_webcam = not self.flip_webcam\n",
    "        if self.flip_webcam:\n",
    "            self.flip_button.setText(\"Unflip\")\n",
    "        else:\n",
    "            self.flip_button.setText(\"Flip Horizontal\")\n",
    "\n",
    "    def update_video_frame(self):\n",
    "        ret, frame = self.cap_video.read()\n",
    "        if not ret:\n",
    "            # End of video\n",
    "            self.video_timer.stop()\n",
    "            self.cap_video.release()\n",
    "            if self.video_out is not None:\n",
    "                self.video_out.release()\n",
    "                self.video_out = None\n",
    "                self.processed_video_output_path = os.path.join(tempfile.gettempdir(), 'processed_video_output.mp4')\n",
    "                QMessageBox.information(self, \"Info\", f\"Finished video playback.\\nProcessed video saved at:\\n{self.processed_video_output_path}\")\n",
    "            return\n",
    "\n",
    "        if self.flip_webcam:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Apply selected filters to the frame\n",
    "        processed_frame = self.apply_filters_to_frame(frame)\n",
    "\n",
    "        # Display the processed frame in the UI\n",
    "        self.show_image(processed_frame)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        if self.video_out is not None:\n",
    "            self.video_out.write(processed_frame)\n",
    "\n",
    "app = QApplication(sys.argv)\n",
    "win = MainWindow()\n",
    "win.show()\n",
    "sys.exit(app.exec_())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
